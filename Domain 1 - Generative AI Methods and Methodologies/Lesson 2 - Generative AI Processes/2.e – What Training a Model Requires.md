# Lesson 2.e â€“ What Training a Model Requires

### ğŸ¯ Lesson Objective
Understand the hardware requirements for training large language models (LLMs), with a focus on power demands and the role of GPUs.

---

## âš¡ Energy Requirements

- Training on billions of data points requires **significant energy consumption**.
- A system designed for training LLMs must include a **robust power supply**.

---

## ğŸ§  The Role of GPUs vs. CPUs

- **GPUs (Graphics Processing Units)** are more critical than CPUs (Central Processing Units) for training generative AI models.

> â€œWhy a GPU first? A GPU can handle parallel processing better than a CPU.â€

### Benefits of GPUs for AI Training

- **Parallel Processing**: GPUs excel at handling multiple tasks simultaneously.
- **Speed**: Process instructions **faster** than CPUs.
- **Visual Support**: Generative AI often works with **images and video**â€”GPU strength.
- **Energy Efficiency**: GPUs are generally **more power-efficient** than CPUs.

---

## ğŸ§Š Practical Benefits for Data Centers

- Due to their efficiency:
  - GPUs help **reduce power consumption**
  - Lower **heat output**, making it easier to manage temperatures
  - Support more **scalable and sustainable infrastructure**

---

## ğŸ’¡ Key Insight

To train modern LLMs effectively, you need not just vast datasetsâ€”but the **right hardware**. In most cases, this means **energy-efficient GPUs** that can handle high-throughput tasks far better than traditional CPUs.

