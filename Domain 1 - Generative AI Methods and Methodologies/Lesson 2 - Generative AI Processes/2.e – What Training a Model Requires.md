# Lesson 2.e – What Training a Model Requires

### 🎯 Lesson Objective
Understand the hardware requirements for training large language models (LLMs), with a focus on power demands and the role of GPUs.

---

## ⚡ Energy Requirements

- Training on billions of data points requires **significant energy consumption**.
- A system designed for training LLMs must include a **robust power supply**.

---

## 🧠 The Role of GPUs vs. CPUs

- **GPUs (Graphics Processing Units)** are more critical than CPUs (Central Processing Units) for training generative AI models.

> “Why a GPU first? A GPU can handle parallel processing better than a CPU.”

### Benefits of GPUs for AI Training

- **Parallel Processing**: GPUs excel at handling multiple tasks simultaneously.
- **Speed**: Process instructions **faster** than CPUs.
- **Visual Support**: Generative AI often works with **images and video**—GPU strength.
- **Energy Efficiency**: GPUs are generally **more power-efficient** than CPUs.

---

## 🧊 Practical Benefits for Data Centers

- Due to their efficiency:
  - GPUs help **reduce power consumption**
  - Lower **heat output**, making it easier to manage temperatures
  - Support more **scalable and sustainable infrastructure**

---

## 💡 Key Insight

To train modern LLMs effectively, you need not just vast datasets—but the **right hardware**. In most cases, this means **energy-efficient GPUs** that can handle high-throughput tasks far better than traditional CPUs.

