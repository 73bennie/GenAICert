# Lesson 2.d – How Image Models Are Trained

### 🎯 Lesson Objective
Understand how image-based generative AI models are trained using text-image pairs, and why manual tagging and prompt specificity are important in generating accurate visual outputs.

---

## 🖼️ Training Image Models

- Image models are trained using **text-image pairs**.
- These pairs are created by assigning descriptive words or phrases to specific images.

> “Image models are trained using text image pairs that are manually tagged.”

---

## 🏷️ What Is Tagging?

- Tagging involves describing an image using multiple relevant terms.
- Example exercise:  
  Looking at an image of a smart thermostat in a living room, descriptive tags might include:
  - `home`
  - `living room`
  - `thermostat`
  - `smart thermostat`
  - `IoT`

> “If we assign those words and phrases to this image, then this image could be returned when someone searches... using these keywords.”

---

## 🛠️ Adobe Firefly Example

- The image referenced in the lesson was created with **Adobe Firefly**.
- The input phrase used:  
  _"smart thermostat inside living room"_
- Resulting image was generated based on that prompt using matched text-image training data.

---

## 🧠 Why Tagging Matters

- **Tagging is subjective**—different people might use different descriptions for the same image.
- Manual tagging introduces human interpretation into the dataset, which affects how image generation behaves later.

---

## 🧭 Practical Guidance for Users

- When requesting images from a generative AI system:
  - Think in **human language**
  - Use **specific and descriptive prompts**
- The more clearly you describe what you want, the more likely the model will return a useful result.

> “The more specific we are when asking for an image to be generated, the more likely we will get the result we want.”
