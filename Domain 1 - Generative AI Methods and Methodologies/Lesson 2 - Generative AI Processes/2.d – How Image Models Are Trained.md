# Lesson 2.d â€“ How Image Models Are Trained

### ğŸ¯ Lesson Objective
Understand how image-based generative AI models are trained using text-image pairs, and why manual tagging and prompt specificity are important in generating accurate visual outputs.

---

## ğŸ–¼ï¸ Training Image Models

- Image models are trained using **text-image pairs**.
- These pairs are created by assigning descriptive words or phrases to specific images.

> â€œImage models are trained using text image pairs that are manually tagged.â€

---

## ğŸ·ï¸ What Is Tagging?

- Tagging involves describing an image using multiple relevant terms.
- Example exercise:  
  Looking at an image of a smart thermostat in a living room, descriptive tags might include:
  - `home`
  - `living room`
  - `thermostat`
  - `smart thermostat`
  - `IoT`

> â€œIf we assign those words and phrases to this image, then this image could be returned when someone searches... using these keywords.â€

---

## ğŸ› ï¸ Adobe Firefly Example

- The image referenced in the lesson was created with **Adobe Firefly**.
- The input phrase used:  
  _"smart thermostat inside living room"_
- Resulting image was generated based on that prompt using matched text-image training data.

---

## ğŸ§  Why Tagging Matters

- **Tagging is subjective**â€”different people might use different descriptions for the same image.
- Manual tagging introduces human interpretation into the dataset, which affects how image generation behaves later.

---

## ğŸ§­ Practical Guidance for Users

- When requesting images from a generative AI system:
  - Think in **human language**
  - Use **specific and descriptive prompts**
- The more clearly you describe what you want, the more likely the model will return a useful result.

> â€œThe more specific we are when asking for an image to be generated, the more likely we will get the result we want.â€
